{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e5a849",
   "metadata": {
    "papermill": {
     "duration": 0.007254,
     "end_time": "2024-05-27T14:09:30.224573",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.217319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Motivation:*\n",
    "\n",
    "### Most forked notebooks had unorganized code structure, import libraries were scattered in different places. So I organized functions into one class, removed unnecessary stuff, in hope it makes the system more understandable and give people flexibility to edit. Suggestions and fixes are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b548ef",
   "metadata": {
    "papermill": {
     "duration": 0.006497,
     "end_time": "2024-05-27T14:09:30.237920",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.231423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c902b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:09:30.252667Z",
     "iopub.status.busy": "2024-05-27T14:09:30.252342Z",
     "iopub.status.idle": "2024-05-27T14:09:30.256700Z",
     "shell.execute_reply": "2024-05-27T14:09:30.255958Z"
    },
    "papermill": {
     "duration": 0.013932,
     "end_time": "2024-05-27T14:09:30.258536",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.244604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/abdurrafae/improved-code-interpretation\n",
    "# https://www.kaggle.com/code/dnyaneshwalwadkar/submission-with-the-best-nb-new-api\n",
    "# https://www.kaggle.com/code/utsavsinghal2604/natural-language-and-code-integration\n",
    "# https://www.kaggle.com/code/yuanwangzhang/updated-code-interpretation-n-repetitions-17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a1026",
   "metadata": {
    "papermill": {
     "duration": 0.006414,
     "end_time": "2024-05-27T14:09:30.271718",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.265304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabe956c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:09:30.289270Z",
     "iopub.status.busy": "2024-05-27T14:09:30.288905Z",
     "iopub.status.idle": "2024-05-27T14:09:30.300522Z",
     "shell.execute_reply": "2024-05-27T14:09:30.299875Z"
    },
    "papermill": {
     "duration": 0.024883,
     "end_time": "2024-05-27T14:09:30.303544",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.278661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "NOTEBOOK_START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439b295",
   "metadata": {
    "papermill": {
     "duration": 0.007466,
     "end_time": "2024-05-27T14:09:30.319839",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.312373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61412e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:09:30.337693Z",
     "iopub.status.busy": "2024-05-27T14:09:30.336835Z",
     "iopub.status.idle": "2024-05-27T14:10:41.680369Z",
     "shell.execute_reply": "2024-05-27T14:10:41.679310Z"
    },
    "papermill": {
     "duration": 71.354954,
     "end_time": "2024-05-27T14:10:41.682801",
     "exception": false,
     "start_time": "2024-05-27T14:09:30.327847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/accelerate-0-29-3/accelerate-0.29.3-py3-none-any.whl -qq\n",
    "!pip install -U /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189f5ab",
   "metadata": {
    "papermill": {
     "duration": 0.006452,
     "end_time": "2024-05-27T14:10:41.696570",
     "exception": false,
     "start_time": "2024-05-27T14:10:41.690118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cf7b8f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-27T14:10:41.712335Z",
     "iopub.status.busy": "2024-05-27T14:10:41.712017Z",
     "iopub.status.idle": "2024-05-27T14:10:53.445649Z",
     "shell.execute_reply": "2024-05-27T14:10:53.444824Z"
    },
    "papermill": {
     "duration": 11.744904,
     "end_time": "2024-05-27T14:10:53.448100",
     "exception": false,
     "start_time": "2024-05-27T14:10:41.703196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95b003",
   "metadata": {
    "papermill": {
     "duration": 0.006467,
     "end_time": "2024-05-27T14:10:53.461766",
     "exception": false,
     "start_time": "2024-05-27T14:10:53.455299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# New API initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd4bc2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:10:53.476616Z",
     "iopub.status.busy": "2024-05-27T14:10:53.476189Z",
     "iopub.status.idle": "2024-05-27T14:10:53.518951Z",
     "shell.execute_reply": "2024-05-27T14:10:53.518115Z"
    },
    "papermill": {
     "duration": 0.052481,
     "end_time": "2024-05-27T14:10:53.520918",
     "exception": false,
     "start_time": "2024-05-27T14:10:53.468437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    PRIVATE = True\n",
    "else:\n",
    "    PRIVATE = False\n",
    "\n",
    "if not PRIVATE:\n",
    "    class train_env():\n",
    "        def __init__(self, randomize=False):\n",
    "            self.randomlize = randomize\n",
    "            \n",
    "            self.df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "            self.df['ground_truth'] = self.df['answer']\n",
    "            self.df['answer'] = -1\n",
    "            \n",
    "            if self.randomlize:\n",
    "                self.df = self.df.reset_index().sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            self.predict_called = True\n",
    "            self.counter = 0\n",
    "            self.len = len(self.df)\n",
    "        \n",
    "        \n",
    "        def iter_test(self):\n",
    "             while self.counter<self.len:\n",
    "                if self.predict_called:\n",
    "                    self.predict_called = False\n",
    "                    yield (self.df.loc[[self.counter]][['id','problem']]),(self.df.loc[[self.counter]][['id','answer']])\n",
    "                else:\n",
    "                    print(\"You must call `predict()` successfully before you can continue with `iter_test()`\")\n",
    "                    yield None \n",
    "                \n",
    "        def predict(self, answer):\n",
    "            self.df.loc[self.counter, ('answer')] = answer['answer'].values[0]\n",
    "            self.predict_called = True\n",
    "            self.counter+=1\n",
    "\n",
    "    env = train_env(randomize=True)\n",
    "    iter_test = env.iter_test()\n",
    "else:\n",
    "    # Set up the evaluation API\n",
    "    import aimo\n",
    "\n",
    "    env = aimo.make_env()\n",
    "    iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffaffd4",
   "metadata": {
    "papermill": {
     "duration": 0.006572,
     "end_time": "2024-05-27T14:10:53.534285",
     "exception": false,
     "start_time": "2024-05-27T14:10:53.527713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b05c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:10:53.550236Z",
     "iopub.status.busy": "2024-05-27T14:10:53.549408Z",
     "iopub.status.idle": "2024-05-27T14:11:09.376325Z",
     "shell.execute_reply": "2024-05-27T14:11:09.375510Z"
    },
    "papermill": {
     "duration": 15.837455,
     "end_time": "2024-05-27T14:11:09.378555",
     "exception": false,
     "start_time": "2024-05-27T14:10:53.541100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 14:10:57.203166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-27 14:10:57.203274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-27 14:10:57.468195: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "QUANT = False\n",
    "USE_PAST_KEY = True\n",
    "SEED = 42\n",
    "MODEL_PATH = \"/kaggle/input/deepseek-math\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_REPETITIONS = 19 if PRIVATE else 4\n",
    "MAX_NEW_TOKENS = 2048 if PRIVATE else 512\n",
    "TIME_LIMIT = 31500 if PRIVATE else 1\n",
    "transformers.set_seed(SEED)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "DEVICE_MAP = [('model.embed_tokens', 0),\n",
    "                 ('model.layers.0', 0),\n",
    "                 ('model.layers.1', 0),\n",
    "                 ('model.layers.2', 0),\n",
    "                 ('model.layers.3', 0),\n",
    "                 ('model.layers.4', 0),\n",
    "                 ('model.layers.5', 0),\n",
    "                 ('model.layers.6', 0),\n",
    "                 ('model.layers.7', 0),\n",
    "                 ('model.layers.8', 0),\n",
    "                 ('model.layers.9', 0),\n",
    "                 ('model.layers.10', 0),\n",
    "                 ('model.layers.11', 0),\n",
    "                 ('model.layers.12', 0),\n",
    "                 ('model.layers.13', 0),\n",
    "                 ('model.layers.14', 0),\n",
    "                 ('model.layers.15', 0),\n",
    "                 ('model.layers.16', 0),\n",
    "                 ('model.layers.17', 0),\n",
    "                 ('model.layers.18', 1),\n",
    "                 ('model.layers.19', 1),\n",
    "                 ('model.layers.20', 1),\n",
    "                 ('model.layers.21', 1),\n",
    "                 ('model.layers.22', 1),\n",
    "                 ('model.layers.23', 1),\n",
    "                 ('model.layers.24', 1),\n",
    "                 ('model.layers.25', 1),\n",
    "                 ('model.layers.26', 1),\n",
    "                 ('model.layers.27', 1),\n",
    "                 ('model.layers.28', 1),\n",
    "                 ('model.layers.29', 1),\n",
    "                ('model.layers.30', 1),\n",
    "                  ('model.layers.31', 1),\n",
    "                 ('model.norm', 1),\n",
    "                 ('lm_head', 1)]\n",
    "\n",
    "DEVICE_MAP = {ii:jj for (ii,jj) in DEVICE_MAP}\n",
    "\n",
    "TEMPERATURE = [0.9, 0.9] # temperature, temperature_coding\n",
    "TOP_P = [1.0, 1.0] # top_p, top_p_coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfec96d",
   "metadata": {
    "papermill": {
     "duration": 0.006703,
     "end_time": "2024-05-27T14:11:09.392402",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.385699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Important Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99afef96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:11:09.407802Z",
     "iopub.status.busy": "2024-05-27T14:11:09.406844Z",
     "iopub.status.idle": "2024-05-27T14:11:09.419007Z",
     "shell.execute_reply": "2024-05-27T14:11:09.418357Z"
    },
    "papermill": {
     "duration": 0.02169,
     "end_time": "2024-05-27T14:11:09.420834",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.399144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(transformers.StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(DEVICE) for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            last_token = input_ids[0][-len(stop):]\n",
    "            if torch.all(torch.eq(stop,last_token)):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8f05bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:11:09.435974Z",
     "iopub.status.busy": "2024-05-27T14:11:09.435698Z",
     "iopub.status.idle": "2024-05-27T14:11:09.490225Z",
     "shell.execute_reply": "2024-05-27T14:11:09.489458Z"
    },
    "papermill": {
     "duration": 0.064502,
     "end_time": "2024-05-27T14:11:09.492154",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.427652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLM_SYSTEM:\n",
    "    \n",
    "    def __init__(self, model_path, device_map, temperature, top_p, prompt_options):\n",
    "        #init llm\n",
    "        self.model, self.tokenizer = self.initialize_llm(model_path, device_map)\n",
    "        #init stop words\n",
    "        self.stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"]\n",
    "        self.stop_words_ids = [self.tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in self.stop_words]\n",
    "        self.stopping_criteria = transformers.StoppingCriteriaList([StoppingCriteriaSub(stops=self.stop_words_ids)])\n",
    "        \n",
    "        self.prompt_options = prompt_options\n",
    "        \n",
    "        self.temperature = temperature[0]\n",
    "        self.top_p = top_p[0]\n",
    "\n",
    "        self.temperature_coding = temperature[1]\n",
    "        self.top_p_coding = top_p[1]\n",
    "\n",
    "   \n",
    "        self.total_results = {}\n",
    "        self.total_answers = {}\n",
    "        self.best_stats = {}\n",
    "        self.total_outputs = {}\n",
    "        self.question_type_counts = {}\n",
    "        self.starting_counts = (2,3)\n",
    "        self.problem_count = 0\n",
    "        \n",
    "        self.already_generated_length = 0\n",
    "        self.code_error = None\n",
    "        self.code_error_count = 0\n",
    "        self.code_output = -1\n",
    "#====================================================================================#\n",
    "    def initialize_llm(self, model_path, device_map):\n",
    "        config = transformers.AutoConfig.from_pretrained(model_path)\n",
    "        config.gradient_checkpointing = True\n",
    "\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        if QUANT:\n",
    "            quantization_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"sequential\",\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=quantization_config,\n",
    "                config=config\n",
    "            )\n",
    "        else:\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "        return model, tokenizer\n",
    "#====================================================================================#    \n",
    "    def predict(self, problem):\n",
    "        self.problem_count += 1\n",
    "        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n",
    "    \n",
    "        if TIME_SPENT>TIME_LIMIT:\n",
    "            return 0\n",
    "\n",
    "        for repetition in tqdm(range(N_REPETITIONS)):\n",
    "            print(f\"\\n\\n\\nQUESTION {self.problem_count} - {repetition} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n",
    "            best, best_count = self.best_stats.get(self.problem_count,(-1,-1))\n",
    "            if best_count>np.sqrt(repetition):\n",
    "                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n",
    "                continue\n",
    "\n",
    "            outputs = self.total_outputs.get(self.problem_count,[])\n",
    "            text_answers, code_answers = self.question_type_counts.get(self.problem_count,self.starting_counts)\n",
    "            results = self.total_results.get(self.problem_count,[])\n",
    "            answers = self.total_answers.get(self.problem_count,[])  \n",
    "\n",
    "            for _ in range(5):\n",
    "                self.flush()\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            try:\n",
    "                self.already_generated_length = 0\n",
    "                self.code_error = None\n",
    "                self.code_error_count = 0\n",
    "                self.code_output = -1\n",
    "                \n",
    "                counts = np.array([text_answers,code_answers])\n",
    "\n",
    "                draw = np.random.choice(self.prompt_options, 1,\n",
    "                              p=counts/counts.sum())\n",
    "\n",
    "                initial_message = draw[0].format(problem,\"{}\")            \n",
    "                prompt = f\"User: {initial_message}\"\n",
    "\n",
    "                prompt_original_length = len(prompt)\n",
    "                print(f\"{repetition}_{prompt}\\n\")\n",
    "\n",
    "                model_inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "                prompt_token_length = len(model_inputs['input_ids'][0])\n",
    "\n",
    "                generation_output = self.model.generate(**model_inputs, \n",
    "                                                   max_new_tokens=MAX_NEW_TOKENS-self.already_generated_length,\n",
    "                                                   return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                   do_sample = True,\n",
    "                                                   temperature = self.temperature,\n",
    "                                                   top_p = self.top_p,\n",
    "                                                   num_return_sequences=1, stopping_criteria = self.stopping_criteria)\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "                decoded_output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                print(f\"{decoded_output[prompt_original_length:]}\\n\")\n",
    "                prompt_original_length += len(decoded_output[prompt_original_length:])\n",
    "                cummulative_code = \"\"\n",
    "\n",
    "                stop_word_cond = False\n",
    "                for stop_word in self.stop_words:\n",
    "                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "\n",
    "\n",
    "                while (stop_word_cond) and (self.already_generated_length<(MAX_NEW_TOKENS)):\n",
    "\n",
    "                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n",
    "                        temperature_inner=self.temperature_coding\n",
    "                        top_p_inner = self.top_p_coding\n",
    "                        prompt = decoded_output\n",
    "                    else:\n",
    "                        temperature_inner=self.temperature\n",
    "                        top_p_inner = self.top_p\n",
    "                        try:\n",
    "                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n",
    "                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n",
    "                            else:\n",
    "                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n",
    "\n",
    "\n",
    "                            cummulative_code+=code_text\n",
    "                            self.code_output, CODE_STATUS = self.process_code(cummulative_code, return_shell_output=True)\n",
    "                            print('CODE RESULTS', self.code_output)\n",
    "\n",
    "                            if self.code_error==self.code_output:\n",
    "                                self.code_error_count+=1\n",
    "                            else:\n",
    "                                self.code_error=self.code_output\n",
    "                                self.code_error_count = 0\n",
    "\n",
    "                            if not CODE_STATUS:\n",
    "                                cummulative_code = cummulative_code[:-len(code_text)]\n",
    "\n",
    "                                if self.code_error_count>=1:\n",
    "                                    print(\"REPEATED ERRORS\")\n",
    "                                    break\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print('ERROR PARSING CODE')\n",
    "                            self.code_output = -1\n",
    "\n",
    "                        if self.code_output!=-1:\n",
    "                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n",
    "                                prompt = decoded_output+'```output\\n'+str(self.code_output)+'\\n```\\n'\n",
    "                            else:\n",
    "                                prompt = decoded_output+'\\n'+str(self.code_output)+'\\n```\\n'\n",
    "                        else:\n",
    "                            prompt = decoded_output\n",
    "                            cummulative_code=\"\"\n",
    "                    model_inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "                    self.already_generated_length =  len(model_inputs['input_ids'][0])-prompt_token_length\n",
    "\n",
    "                    if USE_PAST_KEY:\n",
    "                        old_values = generation_output.past_key_values\n",
    "                    else:\n",
    "                        old_values = None\n",
    "\n",
    "                    generation_output = self.model.generate(**model_inputs, \n",
    "                                                       max_new_tokens=MAX_NEW_TOKENS-self.already_generated_length, \n",
    "                                                       return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                       past_key_values=old_values,\n",
    "                                                       do_sample = True,\n",
    "                                                       temperature = temperature_inner,\n",
    "                                                       top_p = top_p_inner,\n",
    "                                                       num_return_sequences=1, stopping_criteria = self.stopping_criteria)\n",
    "                    if USE_PAST_KEY:\n",
    "                        output_ids = generation_output.sequences[0]\n",
    "                    else:\n",
    "                        output_ids = generation_output[0]\n",
    "                    decoded_output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[prompt_original_length:]}\\n\")\n",
    "                    prompt_original_length+=len(decoded_output[prompt_original_length:])\n",
    "\n",
    "                    stop_word_cond = False\n",
    "                    for stop_word in self.stop_words:\n",
    "                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "\n",
    "                raw_output = self.tokenizer.decode(output_ids[prompt_token_length:], skip_special_tokens=True)\n",
    "                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n",
    "                result_output = self.process_text_output(raw_output)\n",
    "\n",
    "                try:\n",
    "                    self.code_output = round(float(eval(self.code_output))) % 1000\n",
    "                except Exception as e:\n",
    "                    print(e,'final_eval')\n",
    "                    self.code_output = -1\n",
    "            except Exception as e:\n",
    "                print(e,\"5\")\n",
    "                result_output, self.code_output = -1, -1\n",
    "\n",
    "            if self.code_output!=-1:\n",
    "                outputs.append(self.code_output)\n",
    "                code_answers+=1\n",
    "\n",
    "            if result_output!=-1:\n",
    "                outputs.append(result_output)\n",
    "                text_answers+=1\n",
    "\n",
    "            if len(outputs) > 0:\n",
    "                occurences = Counter(outputs).most_common()\n",
    "                print(occurences)\n",
    "                if occurences[0][1] > best_count:\n",
    "                    print(\"GOOD ANSWER UPDATED!\")\n",
    "                    best = occurences[0][0]\n",
    "                    best_count = occurences[0][1]\n",
    "                if occurences[0][1] > 5:\n",
    "                    print(\"ANSWER FOUND!\")\n",
    "                    break\n",
    "\n",
    "            results.append(result_output)\n",
    "            answers.append(self.code_output)\n",
    "\n",
    "            self.best_stats[self.problem_count] = (best, best_count) \n",
    "            self.question_type_counts[self.problem_count] = (text_answers, code_answers)\n",
    "            self.total_outputs[self.problem_count] = outputs\n",
    "\n",
    "            self.total_results[self.problem_count] = results\n",
    "            self.total_answers[self.problem_count] = answers\n",
    "\n",
    "            print(\"code_answers\",code_answers-self.starting_counts[1],\"text_answers\",text_answers-self.starting_counts[0])\n",
    "        return self.best_stats[self.problem_count][0]\n",
    "#====================================================================================#\n",
    "    def flush(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "#====================================================================================#\n",
    "    def naive_parse(self, answer):\n",
    "        out = []\n",
    "        start = False\n",
    "        end = False\n",
    "        for l in reversed(list(answer)):\n",
    "            if l in '0123456789' and not end:\n",
    "                start = True\n",
    "                out.append(l)\n",
    "            else:\n",
    "                if start:\n",
    "                    end = True\n",
    "\n",
    "        out = reversed(out)\n",
    "        return ''.join(out)\n",
    "#====================================================================================#\n",
    "    def return_last_print(self, output, n):\n",
    "        lines = output.strip().split('\\n')\n",
    "        if lines:\n",
    "            return lines[n]\n",
    "        else:\n",
    "            return \"\"\n",
    "#====================================================================================#\n",
    "    def repl(self, match):\n",
    "        if \"real\" not in match.group():\n",
    "            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "        else:\n",
    "            return \"{}{}\".format(match.group()[:-1], ')')\n",
    "#====================================================================================#        \n",
    "    def process_code(self, code, return_shell_output=False):\n",
    "    \n",
    "        code = re.sub(r\"symbols\\([^)]+\\)\", self.repl, code)\n",
    "\n",
    "        if return_shell_output:\n",
    "            code = code.replace('\\n', '\\n    ')\n",
    "                # Add a try...except block\n",
    "            code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n",
    "\n",
    "        if not return_shell_output:\n",
    "            print(code)\n",
    "        with open('code.py', 'w') as fout:\n",
    "            fout.write(code)\n",
    "\n",
    "        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "        try:\n",
    "            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "            return_value = self.return_last_print(shell_output, -1)\n",
    "            print(shell_output)\n",
    "            if return_shell_output:\n",
    "                if return_value=='FAIL':\n",
    "                    CODE_STATUS = False\n",
    "                    return_value = self.return_last_print(shell_output, -2)\n",
    "                    if \"not defined\" in return_value:\n",
    "                        return_value+='\\nTry checking the formatting and imports'\n",
    "                else:\n",
    "                    CODE_STATUS = True\n",
    "                return return_value, CODE_STATUS  \n",
    "            self.code_output = round(float(eval(return_value))) % 1000\n",
    "        except Exception as e:\n",
    "            print(e,'shell_output')\n",
    "            self.code_output = -1\n",
    "\n",
    "        if return_shell_output:\n",
    "            if self.code_output==-1:\n",
    "                CODE_STATUS = False\n",
    "            else:\n",
    "                CODE_STATUS = True\n",
    "            return self.code_output, CODE_STATUS  \n",
    "\n",
    "\n",
    "        return self.code_output\n",
    "#====================================================================================#    \n",
    "    def process_text_output(self, output):\n",
    "        result = output    \n",
    "        try:\n",
    "            result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n",
    "\n",
    "            print('BOXED', result_output)\n",
    "            if not len(result_output):\n",
    "                result_output = self.naive_parse(result)\n",
    "            else:\n",
    "                result_output = result_output[-1]\n",
    "\n",
    "            print('BOXED FINAL', result_output)\n",
    "            if not len(result_output):\n",
    "                result_output = -1\n",
    "\n",
    "            else:\n",
    "                result_output = round(float(eval(result_output))) % 1000\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('ERROR PARSING TEXT')\n",
    "            result_output = -1\n",
    "\n",
    "        return result_output\n",
    "#====================================================================================#\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822284d2",
   "metadata": {
    "papermill": {
     "duration": 0.006537,
     "end_time": "2024-05-27T14:11:09.505922",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.499385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c1b4d",
   "metadata": {
    "papermill": {
     "duration": 0.006472,
     "end_time": "2024-05-27T14:11:09.519013",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.512541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c437508b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:11:09.533916Z",
     "iopub.status.busy": "2024-05-27T14:11:09.533609Z",
     "iopub.status.idle": "2024-05-27T14:11:09.538761Z",
     "shell.execute_reply": "2024-05-27T14:11:09.537907Z"
    },
    "papermill": {
     "duration": 0.014915,
     "end_time": "2024-05-27T14:11:09.540688",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.525773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n",
    "\\\"{}\\\"\n",
    "To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\n",
    "Approach:\"\"\"\n",
    "\n",
    "\n",
    "cot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n",
    "\\\"{}\\\"\n",
    "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n",
    "\n",
    "prompt_options = [code,cot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2891f60",
   "metadata": {
    "papermill": {
     "duration": 0.006479,
     "end_time": "2024-05-27T14:11:09.553822",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.547343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a1bb68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:11:09.568270Z",
     "iopub.status.busy": "2024-05-27T14:11:09.567986Z",
     "iopub.status.idle": "2024-05-27T14:14:17.440520Z",
     "shell.execute_reply": "2024-05-27T14:14:17.439697Z"
    },
    "papermill": {
     "duration": 187.882425,
     "end_time": "2024-05-27T14:14:17.442883",
     "exception": false,
     "start_time": "2024-05-27T14:11:09.560458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcbe13e96e24672a77039fd8a6459b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLM_SYSTEM(MODEL_PATH, DEVICE_MAP, TEMPERATURE, TOP_P, prompt_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83834b",
   "metadata": {
    "papermill": {
     "duration": 0.007129,
     "end_time": "2024-05-27T14:14:17.457425",
     "exception": false,
     "start_time": "2024-05-27T14:14:17.450296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63a0411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:14:17.473447Z",
     "iopub.status.busy": "2024-05-27T14:14:17.472779Z",
     "iopub.status.idle": "2024-05-27T14:14:17.526654Z",
     "shell.execute_reply": "2024-05-27T14:14:17.525492Z"
    },
    "papermill": {
     "duration": 0.064106,
     "end_time": "2024-05-27T14:14:17.528688",
     "exception": false,
     "start_time": "2024-05-27T14:14:17.464582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                            problem\n",
      "0  8ee6f3  The points $\\left(x, y\\right)$ satisfying $((\\...\n",
      "       id  answer\n",
      "0  8ee6f3       0 \n",
      "\n",
      "       id                                            problem\n",
      "1  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...\n",
      "       id  answer\n",
      "1  229ee8       0 \n",
      "\n",
      "       id                                            problem\n",
      "2  5277ed  There exists a unique increasing geometric seq...\n",
      "       id  answer\n",
      "2  5277ed       0 \n",
      "\n",
      "       id                                            problem\n",
      "3  bedda4  Let $ABCD$ be a unit square. Let $P$ be the po...\n",
      "       id  answer\n",
      "3  bedda4       0 \n",
      "\n",
      "       id                                            problem\n",
      "4  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...\n",
      "       id  answer\n",
      "4  430b63       0 \n",
      "\n",
      "       id                                            problem\n",
      "5  82e2a0  Suppose that we roll four 6-sided fair dice wi...\n",
      "       id  answer\n",
      "5  82e2a0       0 \n",
      "\n",
      "       id                                            problem\n",
      "6  2fc4ad  Let the `sparkle' operation on positive intege...\n",
      "       id  answer\n",
      "6  2fc4ad       0 \n",
      "\n",
      "       id                                            problem\n",
      "7  246d26  Each of the three-digits numbers $111$ to $999...\n",
      "       id  answer\n",
      "7  246d26       0 \n",
      "\n",
      "       id                                            problem\n",
      "8  d7e9c9  A function $f: \\mathbb N \\to \\mathbb N$ satisf...\n",
      "       id  answer\n",
      "8  d7e9c9       0 \n",
      "\n",
      "       id                                            problem\n",
      "9  739bc9  For how many positive integers $m$ does the eq...\n",
      "       id  answer\n",
      "9  739bc9       0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test, sample_submission in iter_test:\n",
    "    sample_submission['answer'] = llm.predict(test['problem'].values[0])\n",
    "    env.predict(sample_submission)\n",
    "    print(test)\n",
    "    print(sample_submission, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40afc4",
   "metadata": {
    "papermill": {
     "duration": 0.006999,
     "end_time": "2024-05-27T14:14:17.542920",
     "exception": false,
     "start_time": "2024-05-27T14:14:17.535921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da271d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T14:14:17.558731Z",
     "iopub.status.busy": "2024-05-27T14:14:17.558415Z",
     "iopub.status.idle": "2024-05-27T14:14:17.673695Z",
     "shell.execute_reply": "2024-05-27T14:14:17.672657Z"
    },
    "papermill": {
     "duration": 0.125799,
     "end_time": "2024-05-27T14:14:17.675753",
     "exception": false,
     "start_time": "2024-05-27T14:14:17.549954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('code.py', 'w') as fout:\n",
    "    fout.write(\"print('done')\")\n",
    "\n",
    "batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "try:\n",
    "    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "    print(shell_output)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed50576",
   "metadata": {
    "papermill": {
     "duration": 0.007299,
     "end_time": "2024-05-27T14:14:17.690854",
     "exception": false,
     "start_time": "2024-05-27T14:14:17.683555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4913635,
     "sourceId": 8274980,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4913643,
     "sourceId": 8274989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 295.120676,
   "end_time": "2024-05-27T14:14:21.405062",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-27T14:09:26.284386",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0784daa39ca5458b9a0825547e244a0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11770a02740144aa9a8e76aad1e0de75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "37d8b82d343342d586ec661b97af562a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3a40450881fa4813963a56bc8faf91a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa7eb6a9e6bc445c8aed1247cf5862d2",
       "placeholder": "​",
       "style": "IPY_MODEL_b4f28b08afa74e0391151fa705927dcc",
       "value": " 3/3 [03:05&lt;00:00, 60.17s/it]"
      }
     },
     "6dcbe13e96e24672a77039fd8a6459b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cf9dae97c781425b8c51c39af57a24d1",
        "IPY_MODEL_e9d03f0e88a4474199beacb9d98420e0",
        "IPY_MODEL_3a40450881fa4813963a56bc8faf91a8"
       ],
       "layout": "IPY_MODEL_ca5dc20ccbab41cf8d1aea8bbdc08969"
      }
     },
     "b4f28b08afa74e0391151fa705927dcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ca5dc20ccbab41cf8d1aea8bbdc08969": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf9dae97c781425b8c51c39af57a24d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa177a4a127f405d88bbb89e95dd7aed",
       "placeholder": "​",
       "style": "IPY_MODEL_37d8b82d343342d586ec661b97af562a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "e9d03f0e88a4474199beacb9d98420e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0784daa39ca5458b9a0825547e244a0d",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_11770a02740144aa9a8e76aad1e0de75",
       "value": 3.0
      }
     },
     "fa177a4a127f405d88bbb89e95dd7aed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa7eb6a9e6bc445c8aed1247cf5862d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
