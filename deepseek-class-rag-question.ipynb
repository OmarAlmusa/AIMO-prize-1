{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9039bcf8",
   "metadata": {
    "papermill": {
     "duration": 0.011706,
     "end_time": "2024-06-27T09:35:37.230442",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.218736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *Motivation:*\n",
    "\n",
    "### Most forked notebooks had unorganized code structure, import libraries were scattered in different places. So I organized functions into one class, removed unnecessary stuff, in hope it makes the system more understandable and give people flexibility to edit. Suggestions and fixes are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a96a2",
   "metadata": {
    "papermill": {
     "duration": 0.011055,
     "end_time": "2024-06-27T09:35:37.252857",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.241802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d158f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:35:37.277248Z",
     "iopub.status.busy": "2024-06-27T09:35:37.276489Z",
     "iopub.status.idle": "2024-06-27T09:35:37.281313Z",
     "shell.execute_reply": "2024-06-27T09:35:37.280503Z"
    },
    "papermill": {
     "duration": 0.019142,
     "end_time": "2024-06-27T09:35:37.283306",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.264164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/abdurrafae/improved-code-interpretation\n",
    "# https://www.kaggle.com/code/dnyaneshwalwadkar/submission-with-the-best-nb-new-api\n",
    "# https://www.kaggle.com/code/utsavsinghal2604/natural-language-and-code-integration\n",
    "# https://www.kaggle.com/code/yuanwangzhang/updated-code-interpretation-n-repetitions-17\n",
    "\n",
    "#RAG Creation notebooks:\n",
    "#https://www.kaggle.com/code/anrenk/math-vector-database-creation?scriptVersionId=185610038\n",
    "#https://www.kaggle.com/code/anrenk/creating-vectordb-for-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57468254",
   "metadata": {
    "papermill": {
     "duration": 0.011071,
     "end_time": "2024-06-27T09:35:37.306547",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.295476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb72551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:35:37.331142Z",
     "iopub.status.busy": "2024-06-27T09:35:37.330901Z",
     "iopub.status.idle": "2024-06-27T09:35:37.340826Z",
     "shell.execute_reply": "2024-06-27T09:35:37.340054Z"
    },
    "papermill": {
     "duration": 0.024448,
     "end_time": "2024-06-27T09:35:37.342734",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.318286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "NOTEBOOK_START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d6f99",
   "metadata": {
    "papermill": {
     "duration": 0.011373,
     "end_time": "2024-06-27T09:35:37.365394",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.354021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94013156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:35:37.389063Z",
     "iopub.status.busy": "2024-06-27T09:35:37.388826Z",
     "iopub.status.idle": "2024-06-27T09:37:20.377627Z",
     "shell.execute_reply": "2024-06-27T09:37:20.376566Z"
    },
    "papermill": {
     "duration": 103.003452,
     "end_time": "2024-06-27T09:37:20.380187",
     "exception": false,
     "start_time": "2024-06-27T09:35:37.376735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/accelerate-0-29-3/accelerate-0.29.3-py3-none-any.whl -qq\n",
    "!pip install -U /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl -qq\n",
    "!pip install -U /kaggle/input/sentence-transformers-3-0-1/sentence_transformers-3.0.1-py3-none-any.whl -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009af8f",
   "metadata": {
    "papermill": {
     "duration": 0.011559,
     "end_time": "2024-06-27T09:37:20.404085",
     "exception": false,
     "start_time": "2024-06-27T09:37:20.392526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37d925e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:20.427980Z",
     "iopub.status.busy": "2024-06-27T09:37:20.427670Z",
     "iopub.status.idle": "2024-06-27T09:37:42.612067Z",
     "shell.execute_reply": "2024-06-27T09:37:42.611114Z"
    },
    "papermill": {
     "duration": 22.199559,
     "end_time": "2024-06-27T09:37:42.614733",
     "exception": false,
     "start_time": "2024-06-27T09:37:20.415174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 09:37:33.744557: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-27 09:37:33.744664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-27 09:37:33.898549: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d8576",
   "metadata": {
    "papermill": {
     "duration": 0.011127,
     "end_time": "2024-06-27T09:37:42.637730",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.626603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# New API initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244eda66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:42.662112Z",
     "iopub.status.busy": "2024-06-27T09:37:42.661396Z",
     "iopub.status.idle": "2024-06-27T09:37:42.696553Z",
     "shell.execute_reply": "2024-06-27T09:37:42.695837Z"
    },
    "papermill": {
     "duration": 0.049634,
     "end_time": "2024-06-27T09:37:42.698578",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.648944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    PRIVATE = True\n",
    "else:\n",
    "    PRIVATE = False\n",
    "\n",
    "if not PRIVATE:\n",
    "    class train_env():\n",
    "        def __init__(self, randomize=False):\n",
    "            self.randomlize = randomize\n",
    "            \n",
    "            self.df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "            self.df['ground_truth'] = self.df['answer']\n",
    "            self.df['answer'] = -1\n",
    "            \n",
    "            if self.randomlize:\n",
    "                self.df = self.df.reset_index().sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            self.predict_called = True\n",
    "            self.counter = 0\n",
    "            self.len = len(self.df)\n",
    "        \n",
    "        \n",
    "        def iter_test(self):\n",
    "             while self.counter<self.len:\n",
    "                if self.predict_called:\n",
    "                    self.predict_called = False\n",
    "                    yield (self.df.loc[[self.counter]][['id','problem']]),(self.df.loc[[self.counter]][['id','answer']])\n",
    "                else:\n",
    "                    print(\"You must call `predict()` successfully before you can continue with `iter_test()`\")\n",
    "                    yield None \n",
    "                \n",
    "        def predict(self, answer):\n",
    "            self.df.loc[self.counter, ('answer')] = answer['answer'].values[0]\n",
    "            self.predict_called = True\n",
    "            self.counter+=1\n",
    "\n",
    "    env = train_env(randomize=True)\n",
    "    iter_test = env.iter_test()\n",
    "else:\n",
    "    # Set up the evaluation API\n",
    "    import aimo\n",
    "\n",
    "    env = aimo.make_env()\n",
    "    iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80585349",
   "metadata": {
    "papermill": {
     "duration": 0.011027,
     "end_time": "2024-06-27T09:37:42.721884",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.710857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac94a2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:42.745746Z",
     "iopub.status.busy": "2024-06-27T09:37:42.745466Z",
     "iopub.status.idle": "2024-06-27T09:37:42.839320Z",
     "shell.execute_reply": "2024-06-27T09:37:42.838373Z"
    },
    "papermill": {
     "duration": 0.108173,
     "end_time": "2024-06-27T09:37:42.841222",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.733049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUANT = False\n",
    "USE_PAST_KEY = True\n",
    "MODEL_PATH = \"/kaggle/input/deepseek-math\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_REPETITIONS = 17\n",
    "MAX_NEW_TOKENS = 2048\n",
    "TIME_LIMIT = 31500 if PRIVATE else 1\n",
    "QUESTION_TIME_LIMIT = 540 #9 minutes\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "'''DEVICE_MAP = [('model.embed_tokens', 0),\n",
    "                 ('model.layers.0', 0),\n",
    "                 ('model.layers.1', 0),\n",
    "                 ('model.layers.2', 0),\n",
    "                 ('model.layers.3', 0),\n",
    "                 ('model.layers.4', 0),\n",
    "                 ('model.layers.5', 0),\n",
    "                 ('model.layers.6', 0),\n",
    "                 ('model.layers.7', 0),\n",
    "                 ('model.layers.8', 0),\n",
    "                 ('model.layers.9', 0),\n",
    "                 ('model.layers.10', 0),\n",
    "                 ('model.layers.11', 0),\n",
    "                 ('model.layers.12', 0),\n",
    "                 ('model.layers.13', 0),\n",
    "                 ('model.layers.14', 0),\n",
    "                 ('model.layers.15', 0),\n",
    "                 ('model.layers.16', 0),\n",
    "                 ('model.layers.17', 0),\n",
    "                 ('model.layers.18', 1),\n",
    "                 ('model.layers.19', 1),\n",
    "                 ('model.layers.20', 1),\n",
    "                 ('model.layers.21', 1),\n",
    "                 ('model.layers.22', 1),\n",
    "                 ('model.layers.23', 1),\n",
    "                 ('model.layers.24', 1),\n",
    "                 ('model.layers.25', 1),\n",
    "                 ('model.layers.26', 1),\n",
    "                 ('model.layers.27', 1),\n",
    "                 ('model.layers.28', 1),\n",
    "                 ('model.layers.29', 1),\n",
    "                ('model.layers.30', 1),\n",
    "                  ('model.layers.31', 1),\n",
    "                 ('model.norm', 1),\n",
    "                 ('lm_head', 1)]\n",
    "\n",
    "DEVICE_MAP = {ii:jj for (ii,jj) in DEVICE_MAP}\n",
    "'''\n",
    "TEMPERATURE = [0.9, 0.9] # temperature, temperature_coding\n",
    "TOP_P = [1.0, 1.0] # top_p, top_p_coding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd484d9b",
   "metadata": {
    "papermill": {
     "duration": 0.011149,
     "end_time": "2024-06-27T09:37:42.863953",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.852804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Important Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41342ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:42.887811Z",
     "iopub.status.busy": "2024-06-27T09:37:42.887283Z",
     "iopub.status.idle": "2024-06-27T09:37:42.893547Z",
     "shell.execute_reply": "2024-06-27T09:37:42.892666Z"
    },
    "papermill": {
     "duration": 0.020325,
     "end_time": "2024-06-27T09:37:42.895504",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.875179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(transformers.StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(DEVICE) for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            last_token = input_ids[0][-len(stop):]\n",
    "            if torch.all(torch.eq(stop,last_token)):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e32d221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:42.919332Z",
     "iopub.status.busy": "2024-06-27T09:37:42.919077Z",
     "iopub.status.idle": "2024-06-27T09:37:42.925126Z",
     "shell.execute_reply": "2024-06-27T09:37:42.924360Z"
    },
    "papermill": {
     "duration": 0.020235,
     "end_time": "2024-06-27T09:37:42.927062",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.906827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_llm_model(model_path, QUANT=False):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    if QUANT:\n",
    "        quantization_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"sequential\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config,\n",
    "            #config=config\n",
    "        )\n",
    "    else:\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            #config=config\n",
    "        )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ae1577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:42.951957Z",
     "iopub.status.busy": "2024-06-27T09:37:42.951681Z",
     "iopub.status.idle": "2024-06-27T09:37:42.958849Z",
     "shell.execute_reply": "2024-06-27T09:37:42.958047Z"
    },
    "papermill": {
     "duration": 0.021883,
     "end_time": "2024-06-27T09:37:42.960761",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.938878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_prompt(query, txt_db, vector_db, questions_db, questions_txt_db, embedder, k=3):\n",
    "    \n",
    "    encoded_problem = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    dot_scores = util.dot_score(vector_db, encoded_problem)\n",
    "    topk_outs = torch.topk(dot_scores.squeeze(), k=k)\n",
    "    \n",
    "    questions_dot_scores = util.dot_score(questions_db, encoded_problem)\n",
    "    questions_topk_outs = torch.topk(questions_dot_scores.squeeze(), k=1)\n",
    "    \n",
    "    #print(topk_outs)\n",
    "    \n",
    "    retrieved_data = ''\n",
    "    for page_index in topk_outs.indices:\n",
    "        retrieved_data += '- ' + txt_db[page_index] + '\\n'\n",
    "        \n",
    "    similar_problem = questions_txt_db[questions_topk_outs.indices[0].item()]\n",
    "    similar_problem = similar_problem.replace(\"Answer: \", \"Solution: \")\n",
    "    \n",
    "    retriv = \"Based on the following context:\\n\\n\" + retrieved_data + \"\\n\\nHere is a similar example that can help:\\n\\n\" + similar_problem\n",
    "    return retriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76a04c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:42.985745Z",
     "iopub.status.busy": "2024-06-27T09:37:42.985460Z",
     "iopub.status.idle": "2024-06-27T09:37:43.040210Z",
     "shell.execute_reply": "2024-06-27T09:37:43.039589Z"
    },
    "papermill": {
     "duration": 0.069949,
     "end_time": "2024-06-27T09:37:43.042271",
     "exception": false,
     "start_time": "2024-06-27T09:37:42.972322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLM_SYSTEM:\n",
    "    \n",
    "    def __init__(self, model, tokenizer, prompt_options, txt_db, vector_db, q_txt, q_vector, embedder, temperature, top_p, k=3):\n",
    "        #init llm\n",
    "        #self.model, self.tokenizer = self.initialize_llm(model_path, device_map)\n",
    "        self.model, self.tokenizer = model, tokenizer\n",
    "        \n",
    "        #RAG\n",
    "        self.txt_db = txt_db\n",
    "        self.vector_db = vector_db\n",
    "        self.embedder = embedder\n",
    "        self.k = k\n",
    "        \n",
    "        self.q_txt = q_txt\n",
    "        self.q_vector = q_vector\n",
    "        \n",
    "        #init stop words\n",
    "        self.stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"]\n",
    "        self.stop_words_ids = [self.tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze().to(DEVICE) for stop_word in self.stop_words]\n",
    "        self.stopping_criteria = transformers.StoppingCriteriaList([StoppingCriteriaSub(stops=self.stop_words_ids)])\n",
    "        \n",
    "        \n",
    "        #prompting\n",
    "        self.prompt_options = prompt_options\n",
    "        \n",
    "        self.temperature = temperature[0]\n",
    "        self.top_p = top_p[0]\n",
    "\n",
    "        self.temperature_coding = temperature[1]\n",
    "        self.top_p_coding = top_p[1]\n",
    "\n",
    "   \n",
    "        self.total_results = {}\n",
    "        self.total_answers = {}\n",
    "        self.best_stats = {}\n",
    "        self.total_outputs = {}\n",
    "        self.question_type_counts = {}\n",
    "        self.starting_counts = (2,3)\n",
    "        self.problem_count = 0\n",
    "        \n",
    "        self.already_generated_length = 0\n",
    "        self.code_error = None\n",
    "        self.code_error_count = 0\n",
    "        self.code_output = -1\n",
    "#====================================================================================#   \n",
    "    def predict(self, problem):\n",
    "        self.problem_count += 1\n",
    "        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n",
    "        \n",
    "        question_start_time = time.time()\n",
    "    \n",
    "        if TIME_SPENT>TIME_LIMIT:\n",
    "            return 0\n",
    "\n",
    "        for repetition in tqdm(range(N_REPETITIONS)):\n",
    "            print(f\"\\n\\n\\nQUESTION {self.problem_count} - {repetition} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n",
    "            print(f\"\\nTIME SPENT ON SOLVING THE QUESTION:{time.time()-question_start_time}\\n\\n\")\n",
    "            best, best_count = self.best_stats.get(self.problem_count,(-1,-1))\n",
    "            if best_count>np.sqrt(repetition):\n",
    "                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n",
    "                continue\n",
    "\n",
    "            outputs = self.total_outputs.get(self.problem_count,[])\n",
    "            text_answers, code_answers = self.question_type_counts.get(self.problem_count,self.starting_counts)\n",
    "            results = self.total_results.get(self.problem_count,[])\n",
    "            answers = self.total_answers.get(self.problem_count,[])  \n",
    "\n",
    "            for _ in range(5):\n",
    "                self.flush()\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            try:\n",
    "                self.already_generated_length = 0\n",
    "                self.code_error = None\n",
    "                self.code_error_count = 0\n",
    "                self.code_output = -1\n",
    "                \n",
    "                counts = np.array([text_answers,code_answers])\n",
    "\n",
    "                draw = np.random.choice(self.prompt_options, 1)\n",
    "                              #p=counts/counts.sum())\n",
    "\n",
    "                initial_message = draw[0].format(problem,\"{}\")            \n",
    "                #prompt = f\"User: {initial_message}\"\n",
    "                prompt = initial_message\n",
    "                \n",
    "                #Modify prompt with RAG:\n",
    "                Retrieved_data = augment_prompt(problem, self.txt_db, self.vector_db, self.q_vector, self.q_txt, self.embedder, k=self.k)\n",
    "                \n",
    "                prompt = Retrieved_data + \"\\n\\n\" + prompt\n",
    "                \n",
    "                #print(\"================================PROMPT TEST============================\")\n",
    "                #print(prompt)\n",
    "                #print(\"================================PROMPT TEST============================\")\n",
    "                \n",
    "\n",
    "                prompt_original_length = len(prompt)\n",
    "                print(f\"{repetition}_{prompt}\\n\")\n",
    "\n",
    "                model_inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "                prompt_token_length = len(model_inputs['input_ids'][0])\n",
    "\n",
    "                generation_output = self.model.generate(**model_inputs, \n",
    "                                                   max_new_tokens=MAX_NEW_TOKENS-self.already_generated_length,\n",
    "                                                   return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                   do_sample = True,\n",
    "                                                   temperature = self.temperature,\n",
    "                                                   top_p = self.top_p,\n",
    "                                                   num_return_sequences=1, stopping_criteria = self.stopping_criteria)\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "                decoded_output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                print(f\"{decoded_output[prompt_original_length:]}\\n\")\n",
    "                prompt_original_length += len(decoded_output[prompt_original_length:])\n",
    "                cummulative_code = \"\"\n",
    "\n",
    "                stop_word_cond = False\n",
    "                for stop_word in self.stop_words:\n",
    "                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "\n",
    "\n",
    "                while (stop_word_cond) and (self.already_generated_length<(MAX_NEW_TOKENS)):\n",
    "\n",
    "                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n",
    "                        temperature_inner=self.temperature_coding\n",
    "                        top_p_inner = self.top_p_coding\n",
    "                        prompt = decoded_output\n",
    "                    else:\n",
    "                        temperature_inner=self.temperature\n",
    "                        top_p_inner = self.top_p\n",
    "                        try:\n",
    "                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n",
    "                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n",
    "                            else:\n",
    "                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n",
    "\n",
    "\n",
    "                            cummulative_code+=code_text\n",
    "                            self.code_output, CODE_STATUS = self.process_code(cummulative_code, return_shell_output=True)\n",
    "                            print('CODE RESULTS', self.code_output)\n",
    "\n",
    "                            if self.code_error==self.code_output:\n",
    "                                self.code_error_count+=1\n",
    "                            else:\n",
    "                                self.code_error=self.code_output\n",
    "                                self.code_error_count = 0\n",
    "\n",
    "                            if not CODE_STATUS:\n",
    "                                cummulative_code = cummulative_code[:-len(code_text)]\n",
    "\n",
    "                                if self.code_error_count>=1:\n",
    "                                    print(\"REPEATED ERRORS\")\n",
    "                                    break\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print('ERROR PARSING CODE')\n",
    "                            self.code_output = -1\n",
    "\n",
    "                        if self.code_output!=-1:\n",
    "                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n",
    "                                prompt = decoded_output+'```output\\n'+str(self.code_output)+'\\n```\\n'\n",
    "                            else:\n",
    "                                prompt = decoded_output+'\\n'+str(self.code_output)+'\\n```\\n'\n",
    "                        else:\n",
    "                            prompt = decoded_output\n",
    "                            cummulative_code=\"\"\n",
    "                    model_inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "                    self.already_generated_length =  len(model_inputs['input_ids'][0])-prompt_token_length\n",
    "\n",
    "                    if USE_PAST_KEY:\n",
    "                        old_values = generation_output.past_key_values\n",
    "                    else:\n",
    "                        old_values = None\n",
    "\n",
    "                    generation_output = self.model.generate(**model_inputs, \n",
    "                                                       max_new_tokens=MAX_NEW_TOKENS-self.already_generated_length, \n",
    "                                                       return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                       past_key_values=old_values,\n",
    "                                                       do_sample = True,\n",
    "                                                       temperature = temperature_inner,\n",
    "                                                       top_p = top_p_inner,\n",
    "                                                       num_return_sequences=1, stopping_criteria = self.stopping_criteria)\n",
    "                    if USE_PAST_KEY:\n",
    "                        output_ids = generation_output.sequences[0]\n",
    "                    else:\n",
    "                        output_ids = generation_output[0]\n",
    "                    decoded_output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[prompt_original_length:]}\\n\")\n",
    "                    prompt_original_length+=len(decoded_output[prompt_original_length:])\n",
    "\n",
    "                    stop_word_cond = False\n",
    "                    for stop_word in self.stop_words:\n",
    "                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "\n",
    "                raw_output = self.tokenizer.decode(output_ids[prompt_token_length:], skip_special_tokens=True)\n",
    "                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n",
    "                result_output = self.process_text_output(raw_output)\n",
    "\n",
    "                try:\n",
    "                    self.code_output = round(float(eval(self.code_output))) % 1000\n",
    "                except Exception as e:\n",
    "                    print(e,'final_eval')\n",
    "                    self.code_output = -1\n",
    "            except Exception as e:\n",
    "                print(e,\"5\")\n",
    "                result_output, self.code_output = -1, -1\n",
    "\n",
    "            if self.code_output!=-1:\n",
    "                outputs.append(self.code_output)\n",
    "                code_answers+=1\n",
    "\n",
    "            if result_output!=-1:\n",
    "                outputs.append(result_output)\n",
    "                text_answers+=1\n",
    "\n",
    "            if len(outputs) > 0:\n",
    "                occurences = Counter(outputs).most_common()\n",
    "                print(occurences)\n",
    "                if occurences[0][1] > best_count:\n",
    "                    print(\"GOOD ANSWER UPDATED!\")\n",
    "                    best = occurences[0][0]\n",
    "                    best_count = occurences[0][1]\n",
    "                if occurences[0][1] > 5:\n",
    "                    print(\"ANSWER FOUND!\")\n",
    "                    break\n",
    "\n",
    "            results.append(result_output)\n",
    "            answers.append(self.code_output)\n",
    "\n",
    "            self.best_stats[self.problem_count] = (best, best_count) \n",
    "            self.question_type_counts[self.problem_count] = (text_answers, code_answers)\n",
    "            self.total_outputs[self.problem_count] = outputs\n",
    "\n",
    "            self.total_results[self.problem_count] = results\n",
    "            self.total_answers[self.problem_count] = answers\n",
    "\n",
    "            print(\"code_answers\",code_answers-self.starting_counts[1],\"text_answers\",text_answers-self.starting_counts[0])\n",
    "            \n",
    "            #stop the question if it exceeds 600 seconds\n",
    "            if (time.time() - question_start_time) > QUESTION_TIME_LIMIT:\n",
    "                print(f\"\\n\\nSOLVING QUESTION WAS NOT FINISHED, RETURNING FINAL ANSWER, TIME TAKEN: {time.time()-question_start_time} SECOND(S)\\n\\n\")\n",
    "                break\n",
    "                \n",
    "        if (repetition+1) == N_REPETITIONS:\n",
    "            print(f\"\\n\\nFINISHED SOLVING THE QUESTION IN: {time.time()-question_start_time} SECOND(S)\\n\\n\")\n",
    "        \n",
    "        return self.best_stats[self.problem_count][0]\n",
    "#====================================================================================#\n",
    "    def flush(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "#====================================================================================#\n",
    "    def naive_parse(self, answer):\n",
    "        out = []\n",
    "        start = False\n",
    "        end = False\n",
    "        for l in reversed(list(answer)):\n",
    "            if l in '0123456789' and not end:\n",
    "                start = True\n",
    "                out.append(l)\n",
    "            else:\n",
    "                if start:\n",
    "                    end = True\n",
    "\n",
    "        out = reversed(out)\n",
    "        return ''.join(out)\n",
    "#====================================================================================#\n",
    "    def return_last_print(self, output, n):\n",
    "        lines = output.strip().split('\\n')\n",
    "        if lines:\n",
    "            return lines[n]\n",
    "        else:\n",
    "            return \"\"\n",
    "#====================================================================================#        \n",
    "    def process_code(self, code, return_shell_output=False):\n",
    "        \n",
    "        def repl(match):\n",
    "            if \"real\" not in match.group():\n",
    "                return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "            else:\n",
    "                return \"{}{}\".format(match.group()[:-1], ')')\n",
    "    \n",
    "        code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n",
    "\n",
    "        if return_shell_output:\n",
    "            code = code.replace('\\n', '\\n    ')\n",
    "                # Add a try...except block\n",
    "            code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n",
    "\n",
    "        if not return_shell_output:\n",
    "            print(code)\n",
    "        with open('code.py', 'w') as fout:\n",
    "            fout.write(code)\n",
    "\n",
    "        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "        try:\n",
    "            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "            return_value = self.return_last_print(shell_output, -1)\n",
    "            print(shell_output)\n",
    "            if return_shell_output:\n",
    "                if return_value=='FAIL':\n",
    "                    CODE_STATUS = False\n",
    "                    return_value = self.return_last_print(shell_output, -2)\n",
    "                    if \"not defined\" in return_value:\n",
    "                        return_value+='\\nTry checking the formatting and imports'\n",
    "                else:\n",
    "                    CODE_STATUS = True\n",
    "                return return_value, CODE_STATUS  \n",
    "            self.code_output = round(float(eval(return_value))) % 1000\n",
    "        except Exception as e:\n",
    "            print(e,'shell_output')\n",
    "            self.code_output = -1\n",
    "\n",
    "        if return_shell_output:\n",
    "            if self.code_output==-1:\n",
    "                CODE_STATUS = False\n",
    "            else:\n",
    "                CODE_STATUS = True\n",
    "            return self.code_output, CODE_STATUS  \n",
    "\n",
    "\n",
    "        return self.code_output\n",
    "#====================================================================================#    \n",
    "    def process_text_output(self, output):\n",
    "        result = output    \n",
    "        try:\n",
    "            result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n",
    "\n",
    "            print('BOXED', result_output)\n",
    "            if not len(result_output):\n",
    "                result_output = self.naive_parse(result)\n",
    "            else:\n",
    "                result_output = result_output[-1]\n",
    "\n",
    "            print('BOXED FINAL', result_output)\n",
    "            if not len(result_output):\n",
    "                result_output = -1\n",
    "\n",
    "            else:\n",
    "                result_output = round(float(eval(result_output))) % 1000\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('ERROR PARSING TEXT')\n",
    "            result_output = -1\n",
    "\n",
    "        return result_output\n",
    "#====================================================================================#\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b967457",
   "metadata": {
    "papermill": {
     "duration": 0.011103,
     "end_time": "2024-06-27T09:37:43.065214",
     "exception": false,
     "start_time": "2024-06-27T09:37:43.054111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f12ec5",
   "metadata": {
    "papermill": {
     "duration": 0.011029,
     "end_time": "2024-06-27T09:37:43.087542",
     "exception": false,
     "start_time": "2024-06-27T09:37:43.076513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb0faaa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:43.110906Z",
     "iopub.status.busy": "2024-06-27T09:37:43.110659Z",
     "iopub.status.idle": "2024-06-27T09:37:43.114288Z",
     "shell.execute_reply": "2024-06-27T09:37:43.113462Z"
    },
    "papermill": {
     "duration": 0.017358,
     "end_time": "2024-06-27T09:37:43.116133",
     "exception": false,
     "start_time": "2024-06-27T09:37:43.098775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpnet_path = \"/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "593f14a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:43.139588Z",
     "iopub.status.busy": "2024-06-27T09:37:43.139338Z",
     "iopub.status.idle": "2024-06-27T09:37:47.565668Z",
     "shell.execute_reply": "2024-06-27T09:37:47.564836Z"
    },
    "papermill": {
     "duration": 4.440397,
     "end_time": "2024-06-27T09:37:47.567893",
     "exception": false,
     "start_time": "2024-06-27T09:37:43.127496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=mpnet_path, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc27a4b",
   "metadata": {
    "papermill": {
     "duration": 0.011453,
     "end_time": "2024-06-27T09:37:47.591272",
     "exception": false,
     "start_time": "2024-06-27T09:37:47.579819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load in Databases for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c03fe10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:47.615852Z",
     "iopub.status.busy": "2024-06-27T09:37:47.615538Z",
     "iopub.status.idle": "2024-06-27T09:37:47.633894Z",
     "shell.execute_reply": "2024-06-27T09:37:47.632925Z"
    },
    "papermill": {
     "duration": 0.033576,
     "end_time": "2024-06-27T09:37:47.636123",
     "exception": false,
     "start_time": "2024-06-27T09:37:47.602547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pckl files</th>\n",
       "      <th>pt files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/rag-files/book_10_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_10_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/rag-files/book_11_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_11_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/rag-files/book_12_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_12_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/rag-files/book_13_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_13_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/rag-files/book_14_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_14_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/kaggle/input/rag-files/book_15_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_15_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/kaggle/input/rag-files/book_16_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_16_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/kaggle/input/rag-files/book_17_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_17_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/kaggle/input/rag-files/book_18_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_18_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/kaggle/input/rag-files/book_1_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_1_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/kaggle/input/rag-files/book_2_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_2_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/kaggle/input/rag-files/book_3_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_3_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/kaggle/input/rag-files/book_4_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_4_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/kaggle/input/rag-files/book_5_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_5_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/kaggle/input/rag-files/book_6_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_6_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/kaggle/input/rag-files/book_7_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_7_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/kaggle/input/rag-files/book_8_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_8_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/kaggle/input/rag-files/book_9_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_9_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/kaggle/input/rag-files/full_math_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/full_math_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/kaggle/input/rag-files/geometry_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/geometry_vector_db.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       pckl files  \\\n",
       "0     /kaggle/input/rag-files/book_10_txt_db.pckl   \n",
       "1     /kaggle/input/rag-files/book_11_txt_db.pckl   \n",
       "2     /kaggle/input/rag-files/book_12_txt_db.pckl   \n",
       "3     /kaggle/input/rag-files/book_13_txt_db.pckl   \n",
       "4     /kaggle/input/rag-files/book_14_txt_db.pckl   \n",
       "5     /kaggle/input/rag-files/book_15_txt_db.pckl   \n",
       "6     /kaggle/input/rag-files/book_16_txt_db.pckl   \n",
       "7     /kaggle/input/rag-files/book_17_txt_db.pckl   \n",
       "8     /kaggle/input/rag-files/book_18_txt_db.pckl   \n",
       "9      /kaggle/input/rag-files/book_1_txt_db.pckl   \n",
       "10     /kaggle/input/rag-files/book_2_txt_db.pckl   \n",
       "11     /kaggle/input/rag-files/book_3_txt_db.pckl   \n",
       "12     /kaggle/input/rag-files/book_4_txt_db.pckl   \n",
       "13     /kaggle/input/rag-files/book_5_txt_db.pckl   \n",
       "14     /kaggle/input/rag-files/book_6_txt_db.pckl   \n",
       "15     /kaggle/input/rag-files/book_7_txt_db.pckl   \n",
       "16     /kaggle/input/rag-files/book_8_txt_db.pckl   \n",
       "17     /kaggle/input/rag-files/book_9_txt_db.pckl   \n",
       "18  /kaggle/input/rag-files/full_math_txt_db.pckl   \n",
       "19   /kaggle/input/rag-files/geometry_txt_db.pckl   \n",
       "\n",
       "                                          pt files  \n",
       "0     /kaggle/input/rag-files/book_10_vector_db.pt  \n",
       "1     /kaggle/input/rag-files/book_11_vector_db.pt  \n",
       "2     /kaggle/input/rag-files/book_12_vector_db.pt  \n",
       "3     /kaggle/input/rag-files/book_13_vector_db.pt  \n",
       "4     /kaggle/input/rag-files/book_14_vector_db.pt  \n",
       "5     /kaggle/input/rag-files/book_15_vector_db.pt  \n",
       "6     /kaggle/input/rag-files/book_16_vector_db.pt  \n",
       "7     /kaggle/input/rag-files/book_17_vector_db.pt  \n",
       "8     /kaggle/input/rag-files/book_18_vector_db.pt  \n",
       "9      /kaggle/input/rag-files/book_1_vector_db.pt  \n",
       "10     /kaggle/input/rag-files/book_2_vector_db.pt  \n",
       "11     /kaggle/input/rag-files/book_3_vector_db.pt  \n",
       "12     /kaggle/input/rag-files/book_4_vector_db.pt  \n",
       "13     /kaggle/input/rag-files/book_5_vector_db.pt  \n",
       "14     /kaggle/input/rag-files/book_6_vector_db.pt  \n",
       "15     /kaggle/input/rag-files/book_7_vector_db.pt  \n",
       "16     /kaggle/input/rag-files/book_8_vector_db.pt  \n",
       "17     /kaggle/input/rag-files/book_9_vector_db.pt  \n",
       "18  /kaggle/input/rag-files/full_math_vector_db.pt  \n",
       "19   /kaggle/input/rag-files/geometry_vector_db.pt  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_files_csv = pd.read_csv(\"/kaggle/input/rag-files/rag_files_paths.csv\")\n",
    "rag_files_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0032cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:47.661433Z",
     "iopub.status.busy": "2024-06-27T09:37:47.661151Z",
     "iopub.status.idle": "2024-06-27T09:37:47.667591Z",
     "shell.execute_reply": "2024-06-27T09:37:47.666760Z"
    },
    "papermill": {
     "duration": 0.021382,
     "end_time": "2024-06-27T09:37:47.669806",
     "exception": false,
     "start_time": "2024-06-27T09:37:47.648424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pckl files     /kaggle/input/rag-files/full_math_txt_db.pckl\n",
       "pt files      /kaggle/input/rag-files/full_math_vector_db.pt\n",
       "Name: 18, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_db = rag_files_csv.iloc[18, :]\n",
    "questions_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1e4fead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:47.695252Z",
     "iopub.status.busy": "2024-06-27T09:37:47.694983Z",
     "iopub.status.idle": "2024-06-27T09:37:47.700856Z",
     "shell.execute_reply": "2024-06-27T09:37:47.700039Z"
    },
    "papermill": {
     "duration": 0.020563,
     "end_time": "2024-06-27T09:37:47.702919",
     "exception": false,
     "start_time": "2024-06-27T09:37:47.682356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/input/rag-files/full_math_txt_db.pckl',\n",
       " '/kaggle/input/rag-files/full_math_vector_db.pt')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_txt_db, questions_vector_db = questions_db['pckl files'], questions_db['pt files']\n",
    "questions_txt_db, questions_vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3454f031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:47.727589Z",
     "iopub.status.busy": "2024-06-27T09:37:47.727326Z",
     "iopub.status.idle": "2024-06-27T09:37:47.980345Z",
     "shell.execute_reply": "2024-06-27T09:37:47.979355Z"
    },
    "papermill": {
     "duration": 0.268145,
     "end_time": "2024-06-27T09:37:47.982936",
     "exception": false,
     "start_time": "2024-06-27T09:37:47.714791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(questions_txt_db, 'rb')\n",
    "questions_txt_db = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be879f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:48.009150Z",
     "iopub.status.busy": "2024-06-27T09:37:48.008845Z",
     "iopub.status.idle": "2024-06-27T09:37:48.014239Z",
     "shell.execute_reply": "2024-06-27T09:37:48.013378Z"
    },
    "papermill": {
     "duration": 0.02072,
     "end_time": "2024-06-27T09:37:48.016400",
     "exception": false,
     "start_time": "2024-06-27T09:37:47.995680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36510"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions_txt_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aefe4d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:48.041398Z",
     "iopub.status.busy": "2024-06-27T09:37:48.041049Z",
     "iopub.status.idle": "2024-06-27T09:37:48.996899Z",
     "shell.execute_reply": "2024-06-27T09:37:48.995856Z"
    },
    "papermill": {
     "duration": 0.971276,
     "end_time": "2024-06-27T09:37:48.999571",
     "exception": false,
     "start_time": "2024-06-27T09:37:48.028295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions_vector_db = torch.load(questions_vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9562b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:49.035389Z",
     "iopub.status.busy": "2024-06-27T09:37:49.034622Z",
     "iopub.status.idle": "2024-06-27T09:37:49.040877Z",
     "shell.execute_reply": "2024-06-27T09:37:49.040024Z"
    },
    "papermill": {
     "duration": 0.02525,
     "end_time": "2024-06-27T09:37:49.043077",
     "exception": false,
     "start_time": "2024-06-27T09:37:49.017827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36510, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_vector_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10fe30ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:49.069190Z",
     "iopub.status.busy": "2024-06-27T09:37:49.068919Z",
     "iopub.status.idle": "2024-06-27T09:37:49.086630Z",
     "shell.execute_reply": "2024-06-27T09:37:49.085805Z"
    },
    "papermill": {
     "duration": 0.033081,
     "end_time": "2024-06-27T09:37:49.088791",
     "exception": false,
     "start_time": "2024-06-27T09:37:49.055710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pckl files</th>\n",
       "      <th>pt files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/rag-files/book_10_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_10_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/rag-files/book_11_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_11_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/rag-files/book_12_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_12_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/rag-files/book_13_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_13_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/rag-files/book_14_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_14_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/kaggle/input/rag-files/book_15_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_15_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/kaggle/input/rag-files/book_16_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_16_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/kaggle/input/rag-files/book_17_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_17_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/kaggle/input/rag-files/book_18_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_18_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/kaggle/input/rag-files/book_1_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_1_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/kaggle/input/rag-files/book_2_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_2_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/kaggle/input/rag-files/book_3_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_3_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/kaggle/input/rag-files/book_4_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_4_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/kaggle/input/rag-files/book_5_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_5_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/kaggle/input/rag-files/book_6_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_6_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/kaggle/input/rag-files/book_7_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_7_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/kaggle/input/rag-files/book_8_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_8_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/kaggle/input/rag-files/book_9_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/book_9_vector_db.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/kaggle/input/rag-files/geometry_txt_db.pckl</td>\n",
       "      <td>/kaggle/input/rag-files/geometry_vector_db.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      pckl files  \\\n",
       "0    /kaggle/input/rag-files/book_10_txt_db.pckl   \n",
       "1    /kaggle/input/rag-files/book_11_txt_db.pckl   \n",
       "2    /kaggle/input/rag-files/book_12_txt_db.pckl   \n",
       "3    /kaggle/input/rag-files/book_13_txt_db.pckl   \n",
       "4    /kaggle/input/rag-files/book_14_txt_db.pckl   \n",
       "5    /kaggle/input/rag-files/book_15_txt_db.pckl   \n",
       "6    /kaggle/input/rag-files/book_16_txt_db.pckl   \n",
       "7    /kaggle/input/rag-files/book_17_txt_db.pckl   \n",
       "8    /kaggle/input/rag-files/book_18_txt_db.pckl   \n",
       "9     /kaggle/input/rag-files/book_1_txt_db.pckl   \n",
       "10    /kaggle/input/rag-files/book_2_txt_db.pckl   \n",
       "11    /kaggle/input/rag-files/book_3_txt_db.pckl   \n",
       "12    /kaggle/input/rag-files/book_4_txt_db.pckl   \n",
       "13    /kaggle/input/rag-files/book_5_txt_db.pckl   \n",
       "14    /kaggle/input/rag-files/book_6_txt_db.pckl   \n",
       "15    /kaggle/input/rag-files/book_7_txt_db.pckl   \n",
       "16    /kaggle/input/rag-files/book_8_txt_db.pckl   \n",
       "17    /kaggle/input/rag-files/book_9_txt_db.pckl   \n",
       "18  /kaggle/input/rag-files/geometry_txt_db.pckl   \n",
       "\n",
       "                                         pt files  \n",
       "0    /kaggle/input/rag-files/book_10_vector_db.pt  \n",
       "1    /kaggle/input/rag-files/book_11_vector_db.pt  \n",
       "2    /kaggle/input/rag-files/book_12_vector_db.pt  \n",
       "3    /kaggle/input/rag-files/book_13_vector_db.pt  \n",
       "4    /kaggle/input/rag-files/book_14_vector_db.pt  \n",
       "5    /kaggle/input/rag-files/book_15_vector_db.pt  \n",
       "6    /kaggle/input/rag-files/book_16_vector_db.pt  \n",
       "7    /kaggle/input/rag-files/book_17_vector_db.pt  \n",
       "8    /kaggle/input/rag-files/book_18_vector_db.pt  \n",
       "9     /kaggle/input/rag-files/book_1_vector_db.pt  \n",
       "10    /kaggle/input/rag-files/book_2_vector_db.pt  \n",
       "11    /kaggle/input/rag-files/book_3_vector_db.pt  \n",
       "12    /kaggle/input/rag-files/book_4_vector_db.pt  \n",
       "13    /kaggle/input/rag-files/book_5_vector_db.pt  \n",
       "14    /kaggle/input/rag-files/book_6_vector_db.pt  \n",
       "15    /kaggle/input/rag-files/book_7_vector_db.pt  \n",
       "16    /kaggle/input/rag-files/book_8_vector_db.pt  \n",
       "17    /kaggle/input/rag-files/book_9_vector_db.pt  \n",
       "18  /kaggle/input/rag-files/geometry_vector_db.pt  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_files_csv.drop(18, axis=0, inplace=True)\n",
    "rag_files_csv.reset_index(inplace=True)\n",
    "rag_files_csv.drop(['index'], axis=1, inplace=True)\n",
    "rag_files_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4f2ba9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:49.120596Z",
     "iopub.status.busy": "2024-06-27T09:37:49.120008Z",
     "iopub.status.idle": "2024-06-27T09:37:49.902211Z",
     "shell.execute_reply": "2024-06-27T09:37:49.901049Z"
    },
    "papermill": {
     "duration": 0.798291,
     "end_time": "2024-06-27T09:37:49.904948",
     "exception": false,
     "start_time": "2024-06-27T09:37:49.106657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19/19 [00:00<00:00, 25.61it/s]\n"
     ]
    }
   ],
   "source": [
    "full_math_str = []\n",
    "full_math_vector = []\n",
    "\n",
    "for n in tqdm(range(len(rag_files_csv))):\n",
    "    f = open(rag_files_csv[\"pckl files\"][n], 'rb')\n",
    "    math_txt = pickle.load(f)\n",
    "    full_math_str += math_txt\n",
    "    f.close()\n",
    "\n",
    "    math_vector_db = torch.load(rag_files_csv[\"pt files\"][n])\n",
    "    full_math_vector.append(math_vector_db)\n",
    "    \n",
    "full_math_vector = torch.cat(full_math_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b4fe474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:49.935208Z",
     "iopub.status.busy": "2024-06-27T09:37:49.934896Z",
     "iopub.status.idle": "2024-06-27T09:37:49.940149Z",
     "shell.execute_reply": "2024-06-27T09:37:49.939347Z"
    },
    "papermill": {
     "duration": 0.02197,
     "end_time": "2024-06-27T09:37:49.942158",
     "exception": false,
     "start_time": "2024-06-27T09:37:49.920188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13414"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_math_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8839403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:50.012138Z",
     "iopub.status.busy": "2024-06-27T09:37:50.011794Z",
     "iopub.status.idle": "2024-06-27T09:37:50.017572Z",
     "shell.execute_reply": "2024-06-27T09:37:50.016646Z"
    },
    "papermill": {
     "duration": 0.063947,
     "end_time": "2024-06-27T09:37:50.019510",
     "exception": false,
     "start_time": "2024-06-27T09:37:49.955563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13414, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_math_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355bc26",
   "metadata": {
    "papermill": {
     "duration": 0.016648,
     "end_time": "2024-06-27T09:37:50.052294",
     "exception": false,
     "start_time": "2024-06-27T09:37:50.035646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1607017",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:50.093515Z",
     "iopub.status.busy": "2024-06-27T09:37:50.092570Z",
     "iopub.status.idle": "2024-06-27T09:37:50.099449Z",
     "shell.execute_reply": "2024-06-27T09:37:50.098592Z"
    },
    "papermill": {
     "duration": 0.02771,
     "end_time": "2024-06-27T09:37:50.101447",
     "exception": false,
     "start_time": "2024-06-27T09:37:50.073737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncode = \"\"\"Please solve the following problem step by step:\\n\"{}\"\\nTo accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Your final answer should be positive integer, not an algebraic expression!\\nWrite the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\"\"\"\\n\\n\\ncot = \"\"\"Please solve the following problem step by step:\\n\"{}\"\\nAnalyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\"\"\"\\n\\nprompt_options = [code,cot]\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "code = \"\"\"Please solve the following problem step by step:\n",
    "\\\"{}\\\"\n",
    "To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Your final answer should be positive integer, not an algebraic expression!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cot = \"\"\"Please solve the following problem step by step:\n",
    "\\\"{}\\\"\n",
    "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\"\"\"\n",
    "\n",
    "prompt_options = [code,cot]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5946953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:50.129961Z",
     "iopub.status.busy": "2024-06-27T09:37:50.129657Z",
     "iopub.status.idle": "2024-06-27T09:37:50.136008Z",
     "shell.execute_reply": "2024-06-27T09:37:50.135113Z"
    },
    "papermill": {
     "duration": 0.022976,
     "end_time": "2024-06-27T09:37:50.138019",
     "exception": false,
     "start_time": "2024-06-27T09:37:50.115043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template_1 = \"\"\"\n",
    "Please solve the following problem step by step:\n",
    "\"{}\"\n",
    "To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Your final answer should be a positive integer, not an algebraic expression!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{{}}.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_2 = \"\"\"\n",
    "Please solve the following problem step by step:\n",
    "\"{}\"\n",
    "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{{}}.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_3 = \"\"\"\n",
    "Solve the following problem and provide a detailed explanation along with the solution:\n",
    "\"{}\"\n",
    "Explain your thought process, outline the steps you take, and provide the final numerical answer in \\\\boxed{{}}.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_4 = \"\"\"\n",
    "Use Python to solve the following problem:\n",
    "\"{}\"\n",
    "Write a complete Python script that includes necessary imports, defines functions, and prints the final numerical answer within \\\\boxed{{}}. Ensure that the script is well-documented with comments.\n",
    "\"\"\"\n",
    "\n",
    "#prompt_template_5 = \"\"\"\n",
    "#Prove and solve the following problem:\n",
    "#\"{}\"\n",
    "#Provide a mathematical proof along with the solution steps, and output the final numerical answer in \\\\boxed{{}}.\n",
    "#\"\"\"\n",
    "\n",
    "prompt_options = [prompt_template_1, prompt_template_2, prompt_template_3, prompt_template_4]#, prompt_template_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3a1dd",
   "metadata": {
    "papermill": {
     "duration": 0.013587,
     "end_time": "2024-06-27T09:37:50.165118",
     "exception": false,
     "start_time": "2024-06-27T09:37:50.151531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5da8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:37:50.193413Z",
     "iopub.status.busy": "2024-06-27T09:37:50.192939Z",
     "iopub.status.idle": "2024-06-27T09:40:14.614509Z",
     "shell.execute_reply": "2024-06-27T09:40:14.613668Z"
    },
    "papermill": {
     "duration": 144.438239,
     "end_time": "2024-06-27T09:40:14.617076",
     "exception": false,
     "start_time": "2024-06-27T09:37:50.178837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6685113d47bb41c7a34e5c59825aa51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_model, tokenizer = init_llm_model(MODEL_PATH, QUANT=QUANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d00a4cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:40:14.648565Z",
     "iopub.status.busy": "2024-06-27T09:40:14.648229Z",
     "iopub.status.idle": "2024-06-27T09:40:14.655183Z",
     "shell.execute_reply": "2024-06-27T09:40:14.654491Z"
    },
    "papermill": {
     "duration": 0.024546,
     "end_time": "2024-06-27T09:40:14.657197",
     "exception": false,
     "start_time": "2024-06-27T09:40:14.632651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = LLM_SYSTEM(llm_model, tokenizer, prompt_options, full_math_str, full_math_vector, questions_txt_db, questions_vector_db, embedding_model, TEMPERATURE, TOP_P, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a4b1",
   "metadata": {
    "papermill": {
     "duration": 0.01367,
     "end_time": "2024-06-27T09:40:14.684878",
     "exception": false,
     "start_time": "2024-06-27T09:40:14.671208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3137808a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:40:14.713862Z",
     "iopub.status.busy": "2024-06-27T09:40:14.713565Z",
     "iopub.status.idle": "2024-06-27T09:40:14.764848Z",
     "shell.execute_reply": "2024-06-27T09:40:14.763932Z"
    },
    "papermill": {
     "duration": 0.068588,
     "end_time": "2024-06-27T09:40:14.767647",
     "exception": false,
     "start_time": "2024-06-27T09:40:14.699059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                            problem\n",
      "0  5277ed  There exists a unique increasing geometric seq...\n",
      "       id  answer\n",
      "0  5277ed       0 \n",
      "\n",
      "       id                                            problem\n",
      "1  2fc4ad  Let the `sparkle' operation on positive intege...\n",
      "       id  answer\n",
      "1  2fc4ad       0 \n",
      "\n",
      "       id                                            problem\n",
      "2  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...\n",
      "       id  answer\n",
      "2  430b63       0 \n",
      "\n",
      "       id                                            problem\n",
      "3  d7e9c9  A function $f: \\mathbb N \\to \\mathbb N$ satisf...\n",
      "       id  answer\n",
      "3  d7e9c9       0 \n",
      "\n",
      "       id                                            problem\n",
      "4  8ee6f3  The points $\\left(x, y\\right)$ satisfying $((\\...\n",
      "       id  answer\n",
      "4  8ee6f3       0 \n",
      "\n",
      "       id                                            problem\n",
      "5  739bc9  For how many positive integers $m$ does the eq...\n",
      "       id  answer\n",
      "5  739bc9       0 \n",
      "\n",
      "       id                                            problem\n",
      "6  246d26  Each of the three-digits numbers $111$ to $999...\n",
      "       id  answer\n",
      "6  246d26       0 \n",
      "\n",
      "       id                                            problem\n",
      "7  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...\n",
      "       id  answer\n",
      "7  229ee8       0 \n",
      "\n",
      "       id                                            problem\n",
      "8  82e2a0  Suppose that we roll four 6-sided fair dice wi...\n",
      "       id  answer\n",
      "8  82e2a0       0 \n",
      "\n",
      "       id                                            problem\n",
      "9  bedda4  Let $ABCD$ be a unit square. Let $P$ be the po...\n",
      "       id  answer\n",
      "9  bedda4       0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test, sample_submission in iter_test:\n",
    "    sample_submission['answer'] = llm.predict(test['problem'].values[0])\n",
    "    env.predict(sample_submission)\n",
    "    print(test)\n",
    "    print(sample_submission, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2444cf",
   "metadata": {
    "papermill": {
     "duration": 0.014248,
     "end_time": "2024-06-27T09:40:14.796907",
     "exception": false,
     "start_time": "2024-06-27T09:40:14.782659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "361f864b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T09:40:14.826270Z",
     "iopub.status.busy": "2024-06-27T09:40:14.826003Z",
     "iopub.status.idle": "2024-06-27T09:40:14.938326Z",
     "shell.execute_reply": "2024-06-27T09:40:14.937297Z"
    },
    "papermill": {
     "duration": 0.129315,
     "end_time": "2024-06-27T09:40:14.940322",
     "exception": false,
     "start_time": "2024-06-27T09:40:14.811007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('code.py', 'w') as fout:\n",
    "    fout.write(\"print('done')\")\n",
    "\n",
    "batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "try:\n",
    "    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "    print(shell_output)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97afadc",
   "metadata": {
    "papermill": {
     "duration": 0.013942,
     "end_time": "2024-06-27T09:40:14.968645",
     "exception": false,
     "start_time": "2024-06-27T09:40:14.954703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4798051,
     "sourceId": 8120232,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4913635,
     "sourceId": 8274980,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4913643,
     "sourceId": 8274989,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5255192,
     "sourceId": 8749817,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5266716,
     "sourceId": 8765325,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 22015,
     "sourceId": 26154,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 56578,
     "sourceId": 67859,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 284.56766,
   "end_time": "2024-06-27T09:40:18.649007",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-27T09:35:34.081347",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "110e0eb6f9c74ed8829c8c88b16edc3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "175f26fab1ab4abc90cd786e7802df68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18f0fc6180484deb86db32541f1a54f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "426fb8c68338439f8a71eba88d1b31fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4361fba6f8d842b2995567608c0d922c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4cd2795cb00d4d08ac1ecb67005353c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6685113d47bb41c7a34e5c59825aa51c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7e645ad0f0644c98a86761fd9fe20b22",
        "IPY_MODEL_c2ce991d682b4731afb937631b51a548",
        "IPY_MODEL_e238c1d6f299415f85a0a7c194af9ecf"
       ],
       "layout": "IPY_MODEL_426fb8c68338439f8a71eba88d1b31fa"
      }
     },
     "7e645ad0f0644c98a86761fd9fe20b22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_175f26fab1ab4abc90cd786e7802df68",
       "placeholder": "",
       "style": "IPY_MODEL_18f0fc6180484deb86db32541f1a54f4",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "c2ce991d682b4731afb937631b51a548": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4cd2795cb00d4d08ac1ecb67005353c5",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d525f9c51b454cd7bffd71221de3d475",
       "value": 3.0
      }
     },
     "d525f9c51b454cd7bffd71221de3d475": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e238c1d6f299415f85a0a7c194af9ecf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_110e0eb6f9c74ed8829c8c88b16edc3e",
       "placeholder": "",
       "style": "IPY_MODEL_4361fba6f8d842b2995567608c0d922c",
       "value": " 3/3 [02:23&lt;00:00, 47.02s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
